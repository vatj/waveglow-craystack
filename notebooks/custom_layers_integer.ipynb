{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertible Convolution and WaveNet Custom Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "Start with standard imports as well as adding the scripts directory to the system path to allow custom imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_dir, _ = os.path.split(os.getcwd())\n",
    "script_dir = os.path.join(root_dir, 'scripts')\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hparams import hparams\n",
    "from training_utils import round_ste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertible Convolution where the kernel is a permutation matrix and is not trainable.\n",
    "\n",
    "The forward boolean in the call method can be used to run the layer in reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inv1x1ConvPermute(layers.Layer):\n",
    "  \"\"\"\n",
    "  The kernel of this convolution is just a permutation\n",
    "  matrix. The permutation is initialised and kept fixed\n",
    "  during training and evaluation. Contrary to continuous\n",
    "  waveglow there is no added loss.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, filters, seed, **kwargs):\n",
    "    super(Inv1x1ConvPermute, self).__init__(**kwargs)\n",
    "    self.filters = filters\n",
    "    self.seed = seed\n",
    "    \n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    \n",
    "    self.kernel = self.add_weight(\n",
    "      shape=[self.filters, self.filters],\n",
    "      initializer=\"identity\", \n",
    "      trainable=False,\n",
    "      name=\"kernel\")\n",
    "    \n",
    "    # Random permutation. Some combinations lead to a more or less stable training process\n",
    "    self.kernel = tf.expand_dims(tf.random.shuffle(self.kernel, seed=self.seed), axis=0)\n",
    "    \n",
    "    self.inverse_kernel = tf.cast(tf.linalg.inv(\n",
    "          tf.cast(self.kernel, tf.float64)), dtype=self.dtype)\n",
    "    \n",
    "    self.built = True\n",
    "  \n",
    "  def call(self, inputs, forward=True):\n",
    "    if forward:\n",
    "      return tf.nn.conv1d(inputs, self.kernel, \n",
    "                          stride=1, padding='SAME')\n",
    "      \n",
    "    else:\n",
    "      return tf.nn.conv1d(inputs, self.inverse_kernel, \n",
    "                          stride=1, padding='SAME')\n",
    "    \n",
    "  def get_config(self):\n",
    "    config = super(Inv1x1ConvPermute, self).get_config()\n",
    "    config.update(filters = self.filters)\n",
    "    config.update(seed = self.seed)\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia WaveNet Implementation\n",
    "Difference with the original implementations :\n",
    "WaveNet convonlution need not be causal. \n",
    "No dilation size reset. \n",
    "Dilation doubles on each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetNvidia(layers.Layer):\n",
    "  \"\"\"\n",
    "  Wavenet Block as defined in the WaveGlow implementation from Nvidia\n",
    "  \n",
    "  WaveNet convonlution need not be causal. \n",
    "  No dilation size reset. \n",
    "  Dilation doubles on each layer.\n",
    "  \"\"\"\n",
    "  def __init__(self, n_in_channels, n_channels = 256, \n",
    "               n_layers = 12, kernel_size = 3, **kwargs):\n",
    "    super(WaveNetNvidia, self).__init__(**kwargs)\n",
    "    \n",
    "    assert(kernel_size % 2 == 1)\n",
    "    assert(n_channels % 2 == 0)\n",
    "    \n",
    "    self.n_layers = n_layers\n",
    "    self.n_channels = n_channels\n",
    "    self.n_in_channels = n_in_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    \n",
    "    self.in_layers = []\n",
    "    self.res_skip_layers = []\n",
    "    \n",
    "    self.start = layers.Conv1D(filters=self.n_channels,\n",
    "                               kernel_size=1,\n",
    "                               dtype=self.dtype,\n",
    "                               name=\"start\")\n",
    "    \n",
    "    self.end = layers.Conv1D(\n",
    "      filters=3 * self.n_in_channels,\n",
    "      kernel_size = 1,\n",
    "      kernel_initializer=tf.initializers.zeros(),\n",
    "      bias_initializer=tf.initializers.zeros(),\n",
    "      activation=tf.nn.tanh,\n",
    "      dtype=self.dtype,\n",
    "      name=\"end\")\n",
    "\n",
    "    for index in range(self.n_layers):\n",
    "      dilation_rate = 2 ** index\n",
    "      in_layer = layers.Conv1D(filters=2 * self.n_channels,\n",
    "                    kernel_size= self.kernel_size,\n",
    "                    dilation_rate=dilation_rate,\n",
    "                    padding=\"SAME\",\n",
    "                    dtype=self.dtype,\n",
    "                    name=\"conv1D_{}\".format(index))\n",
    "     \n",
    "      self.in_layers.append(in_layer)\n",
    "      \n",
    "      if index < self.n_layers - 1:\n",
    "        res_skip_channels = 2 * self.n_channels\n",
    "      else:\n",
    "        res_skip_channels = self.n_channels\n",
    "        \n",
    "      res_skip_layer = layers.Conv1D(\n",
    "        filters=res_skip_channels,\n",
    "        kernel_size=1,\n",
    "        dtype=self.dtype,\n",
    "        name=\"res_skip_{}\".format(index))\n",
    "      \n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "      \n",
    "    \n",
    "  def call(self, inputs, training=False):\n",
    "    \"\"\"\n",
    "    This implementatation does not require exposing a training boolean flag \n",
    "    as only the integer coupling behaviour needs reversing during\n",
    "    inference.\n",
    "    \"\"\"\n",
    "    audio_0 = inputs\n",
    "    \n",
    "    started = self.start(audio_0)\n",
    "    \n",
    "    \n",
    "    for index in range(self.n_layers):\n",
    "      in_layered = self.in_layers[index](started)\n",
    "      \n",
    "      half_tanh, half_sigmoid = tf.split(\n",
    "        in_layered, 2, axis=2)\n",
    "      half_tanh = tf.nn.tanh(half_tanh)\n",
    "      half_sigmoid = tf.nn.sigmoid(half_sigmoid)\n",
    "    \n",
    "      activated = half_tanh * half_sigmoid\n",
    "      \n",
    "      res_skip_activation = self.res_skip_layers[index](activated)\n",
    "      \n",
    "      if index < (self.n_layers - 1):\n",
    "        res_skip_activation_0, res_skip_activation_1 = tf.split(\n",
    "          res_skip_activation, 2, axis=2)\n",
    "        started = res_skip_activation_0 + started\n",
    "        skip_activation = res_skip_activation_1\n",
    "      else:\n",
    "        skip_activation = res_skip_activation\n",
    "\n",
    "      if index == 0:\n",
    "        output = skip_activation\n",
    "      else:\n",
    "        output = skip_activation + output\n",
    "        \n",
    "    output = self.end(output)\n",
    "    \n",
    "    # Added rounding operation with straight through gradient (ste)\n",
    "    output = round_ste(output * pow(2, 15)) / pow(2, 15)\n",
    "    \n",
    "    return output\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(WaveNetBlock, self).get_config()\n",
    "    config.update(n_in_channels = self.n_in_channels)\n",
    "    config.update(n_channels = self.n_channels)\n",
    "    config.update(n_layers = self.n_layers)\n",
    "    config.update(kernel_size = self.kernel_size)\n",
    "  \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer Translation Coupling Layer\n",
    "\n",
    "In the IDF paper, the affine coupling layer is replaced by a additive layer which differ in a couple of ways:\n",
    "- Rounding operation applied to the output of the wavenet neural network\n",
    "- No multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerCoupling(layers.Layer):\n",
    "  \"\"\"\n",
    "  Invertible Integer Coupling Layer.\n",
    "  Since inputs are between -1,1 we \n",
    "  The inverted behaviour is obtained by setting the forward boolean\n",
    "  in the call method to false.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, **kwargs):\n",
    "    super(IntegerCoupling, self).__init__(**kwargs)\n",
    "    \n",
    "  def call(self, inputs, forward=True):\n",
    "    \n",
    "    audio_1, wavenet_output = inputs\n",
    "    \n",
    "    if forward:\n",
    "      audio_1 = (audio_1 + wavenet_output) % 2 - tf.ones(audio_1.shape, dtype=audio_1.dtype)\n",
    "    else:\n",
    "      audio_1 = (audio_1 - wavenet_output) % 2 - tf.ones(audio_1.shape, dtype=audio_1.dtype)     \n",
    "        \n",
    "    return audio_1\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(IntegerCoupling, self).get_config()\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet And Integer Coupling\n",
    "This block is a convenience block which has been defined to make it more straightforward to implement the WaveGlow model using the keras functional API. This version uses: \n",
    "- Integer coupling instead of the original affine coupling\n",
    "- The conditioning is applied to 25% of the inputs instead of the original 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetIntegerBlock(layers.Layer):\n",
    "  \"\"\"\n",
    "  Wavenet + Integer Coupling Layer\n",
    "  Convenience block to provide a tidy model definition\n",
    "  \"\"\"\n",
    "  def __init__(self, n_in_channels, n_channels = 256,\n",
    "               n_layers = 12, kernel_size = 3, **kwargs):\n",
    "    super(WaveNetIntegerBlock, self).__init__(**kwargs)\n",
    "    \n",
    "    self.n_layers =  n_layers\n",
    "    self.n_channels = n_channels\n",
    "    self.n_in_channels = n_in_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    \n",
    "    self.wavenet = WaveNetNvidia(n_in_channels=n_in_channels,\n",
    "                                 n_channels=n_channels,\n",
    "                                 n_layers=n_layers,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 dtype=self.dtype)\n",
    "    \n",
    "    self.integer_coupling = IntegerCoupling(dtype=self.dtype)\n",
    "      \n",
    "    \n",
    "  def call(self, inputs, forward=True, training=False):\n",
    "    \"\"\"\n",
    "    forward should be set to false to inverse integer layer\n",
    "    Split audio 75%-25% to make it \n",
    "    \"\"\"\n",
    "    \n",
    "    splits = [self.n_in_channels,\n",
    "              3 * self.n_in_channels]\n",
    "    \n",
    "    audio_0, audio_1 = tf.split(inputs, splits, axis=2)\n",
    "    \n",
    "    wavenet_output = self.wavenet(audio_0, training=training)\n",
    "    \n",
    "    audio_1 = self.integer_coupling(\n",
    "      (audio_1, wavenet_output), forward=forward)   \n",
    "         \n",
    "    audio = layers.Concatenate(\n",
    "      axis=2, \n",
    "      dtype=self.integer_coupling.dtype) ([audio_0, audio_1])\n",
    "    \n",
    "    return audio\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(WaveNetIntegerBlock, self).get_config()\n",
    "    config.update(n_in_channels = self.n_in_channels)\n",
    "    config.update(n_channels = self.n_channels)\n",
    "    config.update(n_layers = self.n_layers)\n",
    "    config.update(kernel_size = self.kernel_size)\n",
    "  \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorOutLayer(layers.Layer):\n",
    "  \"\"\"\n",
    "  Factor Out layer implementation.\n",
    "  TODO :\n",
    "  - Implement training boolean\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, n_remaining_channels, n_early_size,\n",
    "               **kwargs):\n",
    "    super(FactorOutLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    self.n_remaining_channels = n_remaining_channels\n",
    "    self.n_early_size = n_early_size\n",
    "    \n",
    "    \n",
    "    \n",
    "  def call(self, audio, forward=True):\n",
    "    \n",
    "    if forward is True:     \n",
    "      audio = layers.Permute(dims=(2, 1), dtype=self.dtype) (audio)\n",
    "\n",
    "      output_chunk = layers.Cropping1D(\n",
    "        cropping=(0, self.n_remaining_channels),\n",
    "        dtype=self.dtype) (audio)\n",
    "\n",
    "      audio = layers.Cropping1D(\n",
    "        cropping=(self.n_early_size, 0),\n",
    "        dtype=self.dtype) (audio)\n",
    "\n",
    "      audio = layers.Permute(dims=(2, 1), dtype=self.dtype) (audio)\n",
    "\n",
    "      output_chunk = layers.Permute(dims=(2, 1), \n",
    "                                    dtype=self.dtype) (output_chunk)\n",
    "      \n",
    "      output_chunk = tf.reshape(output_chunk, \n",
    "                                [output_chunk.shape[0], \n",
    "                                output_chunk.shape[1] * output_chunk.shape[2], \n",
    "                                1])\n",
    "      \n",
    "      return audio, output_chunk\n",
    "    else:\n",
    "      raise NotImplementedError('The false forward boolean for this layer is not working yet')\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(FactorOutLayer, self).get_config()\n",
    "    config.update(n_remaining_channels = self.n_remaining_chanels)\n",
    "    config.update(n_early_size = self.n_early_size)\n",
    "    \n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteLogisticMixParametersWaveNet(layers.Layer):\n",
    "  \"\"\"\n",
    "  Wavenet Block based on Nvidia implementation. \n",
    "  Modified to output logistic mixture parameters\n",
    "  No rounding operation\n",
    "  \n",
    "  WaveNet convonlution need not be causal. \n",
    "  No dilation size reset. \n",
    "  Dilation doubles on each layer.\n",
    "  \"\"\"\n",
    "  def __init__(self, n_factorized_channels, n_logistic_in_mixture,\n",
    "               n_channels = 256, n_layers = 12, kernel_size = 3, **kwargs):\n",
    "    super(DiscreteLogisticMixParametersWaveNet, self).__init__(**kwargs)\n",
    "    \n",
    "    assert(kernel_size % 2 == 1)\n",
    "    assert(n_channels % 2 == 0)\n",
    "    \n",
    "    self.n_layers = n_layers\n",
    "    self.n_logistic_in_mixture = n_logistic_in_mixture\n",
    "    self.n_channels = n_channels\n",
    "    self.n_factorized_channels = n_factorized_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    \n",
    "    self.in_layers = []\n",
    "    self.res_skip_layers = []\n",
    "    \n",
    "    self.start = layers.Conv1D(filters=self.n_channels,\n",
    "                               kernel_size=1,\n",
    "                               dtype=self.dtype,\n",
    "                               name=\"start\")\n",
    "    \n",
    "    self.end = layers.Conv1D(\n",
    "      filters=3*n_factorized_channels*n_logistic_in_mixture,\n",
    "      kernel_size = 1,\n",
    "      kernel_initializer=tf.initializers.GlorotUniform(),\n",
    "      bias_initializer=tf.initializers.zeros(),\n",
    "      dtype=self.dtype,\n",
    "      name=\"end\")\n",
    "\n",
    "    for index in range(self.n_layers):\n",
    "      dilation_rate = 2 ** index\n",
    "      in_layer = layers.Conv1D(filters=2 * self.n_channels,\n",
    "                    kernel_size= self.kernel_size,\n",
    "                    dilation_rate=dilation_rate,\n",
    "                    padding=\"SAME\",\n",
    "                    dtype=self.dtype,\n",
    "                    name=\"conv1D_{}\".format(index))\n",
    "     \n",
    "      self.in_layers.append(in_layer)\n",
    "      \n",
    "      if index < self.n_layers - 1:\n",
    "        res_skip_channels = 2 * self.n_channels\n",
    "      else:\n",
    "        res_skip_channels = self.n_channels\n",
    "        \n",
    "      res_skip_layer = layers.Conv1D(\n",
    "        filters=res_skip_channels,\n",
    "        kernel_size=1,\n",
    "        dtype=self.dtype,\n",
    "        name=\"res_skip_{}\".format(index))\n",
    "      \n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "      \n",
    "    \n",
    "  def call(self, inputs, training=False):\n",
    "    \n",
    "    started = self.start (inputs)\n",
    "    \n",
    "    for index in range(self.n_layers):\n",
    "      in_layered = self.in_layers[index](started)\n",
    "      \n",
    "      half_tanh, half_sigmoid = tf.split(\n",
    "        in_layered, 2, axis=2)\n",
    "      half_tanh = tf.nn.tanh(half_tanh)\n",
    "      half_sigmoid = tf.nn.sigmoid(half_sigmoid)\n",
    "    \n",
    "      activated = half_tanh * half_sigmoid\n",
    "      \n",
    "      res_skip_activation = self.res_skip_layers[index](activated)\n",
    "      \n",
    "      if index < (self.n_layers - 1):\n",
    "        res_skip_activation_0, res_skip_activation_1 = tf.split(\n",
    "          res_skip_activation, 2, axis=2)\n",
    "        started = res_skip_activation_0 + started\n",
    "        skip_activation = res_skip_activation_1\n",
    "      else:\n",
    "        skip_activation = res_skip_activation\n",
    "\n",
    "      if index == 0:\n",
    "        output = skip_activation\n",
    "      else:\n",
    "        output = skip_activation + output\n",
    "        \n",
    "    output = self.end (output)\n",
    "    \n",
    "    logits, means, log_scales = tf.split(output, 3, axis=2)\n",
    "    \n",
    "    target_shape = [logits.shape[0], logits.shape[1] * self.n_factorized_channels, self.n_logistic_in_mixture]\n",
    "    \n",
    "    logits = tf.reshape(logits, target_shape)\n",
    "    means = tf.reshape(means, target_shape)\n",
    "    log_scales = tf.reshape(log_scales, target_shape)\n",
    "    \n",
    "    return logits, means, log_scales\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(WaveNetBlock, self).get_config()\n",
    "    config.update(n_factorized_channels = self.n_factorized_channels)\n",
    "    config.update(n_logistic_in_mixture = self.n_logistic_in_mixture)\n",
    "    config.update(n_channels = self.n_channels)\n",
    "    config.update(n_layers = self.n_layers)\n",
    "    config.update(kernel_size = self.kernel_size)\n",
    "  \n",
    "    return config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waveglow-compression",
   "language": "python",
   "name": "waveglow-compression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
