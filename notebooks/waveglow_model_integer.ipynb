{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_dir, _ = os.path.split(os.getcwd())\n",
    "script_dir = os.path.join(root_dir, 'scripts')\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hparams import hparams\n",
    "from custom_layers_integer import Inv1x1ConvPermute, WaveNetIntegerBlock\n",
    "from custom_layers_integer import DiscreteLogisticMixParametersWaveNet, FactorOutLayer\n",
    "from training_utils import log_sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveGlowInteger(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  Waveglow implementation using the Invertible1x1Conv custom layer and \n",
    "  the WaveNet custom block \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, hparams, **kwargs):\n",
    "    super(WaveGlowInteger, self).__init__(dtype=hparams['ftype'], **kwargs)\n",
    "    \n",
    "    assert(hparams['n_group'] % 2 == 0)\n",
    "    self.n_flows = hparams['n_flows']\n",
    "    self.n_group = hparams['n_group']\n",
    "    self.n_early_every = hparams['n_early_every']\n",
    "    self.n_early_size = hparams['n_early_size']\n",
    "    self.upsampling_size = hparams['upsampling_size']\n",
    "    self.hidden_channels = hparams['hidden_channels']\n",
    "    self.hparams = hparams\n",
    "    self.normalisation = hparams['train_batch_size'] * hparams['segment_length']\n",
    "    self.batch_size = hparams['train_batch_size']\n",
    "    self.seed = hparams['seed']\n",
    "    \n",
    "    # Added for IDF\n",
    "    self.seeds = tf.random.uniform([self.n_flows], maxval=pow(2,16), seed=self.seed, dtype=tf.int32)\n",
    "    self.n_logistic_in_mixture = hparams[\"n_logistic_in_mixture\"]\n",
    "    self.blocks = int(self.n_flows / self.n_early_every)\n",
    "    self.n_factorized_channels = self.n_early_size\n",
    "    self.xdim = hparams['xdim']\n",
    "    self.zdim = hparams['zdim']\n",
    "    self.compressing = False\n",
    "    self.last_log_shift = hparams['last_log_scale']\n",
    "\n",
    "    self.waveNetIntegerBlocks = []\n",
    "    self.inv1x1ConvPermuteLayers = []\n",
    "    \n",
    "    # Added for IDF\n",
    "    self.factorOutLayers = []\n",
    "    self.discreteLogisticMixParametersNets = []\n",
    "      \n",
    "    n_fourth = self.n_group // 4\n",
    "    n_remaining_channels = self.n_group\n",
    "    block = 0\n",
    "    \n",
    "    for index in range(self.n_flows):\n",
    "      if ((index % self.n_early_every == 0) and (index > 0)):\n",
    "        n_fourth -= self.n_early_size // 4\n",
    "        n_remaining_channels -= self.n_early_size\n",
    "        \n",
    "        self.factorOutLayers.append(\n",
    "          FactorOutLayer(\n",
    "            n_remaining_channels = n_remaining_channels,\n",
    "            n_early_size = self.n_early_size))\n",
    "        \n",
    "        self.discreteLogisticMixParametersNets.append(\n",
    "          DiscreteLogisticMixParametersWaveNet(\n",
    "            n_factorized_channels=self.n_early_size,\n",
    "            n_logistic_in_mixture=self.n_logistic_in_mixture,\n",
    "            n_channels=hparams['n_channels'],\n",
    "            n_layers=hparams['n_layers'],\n",
    "            kernel_size=hparams['kernel_size'],\n",
    "            dtype=hparams['ftype']))\n",
    "        \n",
    "        block += 1\n",
    "        \n",
    "    \n",
    "      self.inv1x1ConvPermuteLayers.append(\n",
    "          Inv1x1ConvPermute(\n",
    "            filters=n_remaining_channels,\n",
    "            seed=self.seeds[index],\n",
    "            dtype=hparams['ftype'],\n",
    "            name=\"newInv1x1conv_{}\".format(index)))\n",
    "      \n",
    "      self.waveNetIntegerBlocks.append(\n",
    "        WaveNetIntegerBlock(n_in_channels=n_fourth, \n",
    "                     n_channels=hparams['n_channels'],\n",
    "                     n_layers=hparams['n_layers'],\n",
    "                     kernel_size=hparams['kernel_size'],\n",
    "                     dtype=hparams['ftype'],\n",
    "                     name=\"waveNetIntegerBlock_{}\".format(index)))\n",
    "      \n",
    "      \n",
    "    self.n_remaining_channels = n_remaining_channels\n",
    "    self.n_blocks = block\n",
    "    \n",
    "    \n",
    "  def call(self, inputs, training=None):\n",
    "    \"\"\"\n",
    "    Evaluate model against inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    audio = inputs['wav']\n",
    "    \n",
    "    audio = layers.Reshape(\n",
    "      target_shape = [self.hparams[\"segment_length\"] // self.n_group,\n",
    "                      self.n_group],\n",
    "      dtype=self.dtype) (audio)\n",
    "    \n",
    "    output_audio = []\n",
    "    output_means = []\n",
    "    output_log_scales = []\n",
    "    output_logit = []\n",
    "    \n",
    "    n_remaining_channels = self.n_group\n",
    "    block = 0\n",
    "    \n",
    "    for index in range(self.n_flows):\n",
    "      \n",
    "      if ((index % self.n_early_every == 0) and (index > 0)):\n",
    "        n_remaining_channels -= self.n_early_size\n",
    "        \n",
    "        audio, output_chunk = self.factorOutLayers[block](audio, forward=True)\n",
    "        \n",
    "        logit, means, log_scales = self.discreteLogisticMixParametersNets[block](audio, training=training)\n",
    "        \n",
    "        output_audio.append(output_chunk)\n",
    "        output_logit.append(logit)\n",
    "        output_means.append(means)\n",
    "        output_log_scales.append(log_scales)\n",
    "        block += 1\n",
    "        \n",
    "      audio = self.inv1x1ConvPermuteLayers[index](audio)\n",
    "      audio = self.waveNetIntegerBlocks[index](audio, \n",
    "                                               forward=True,\n",
    "                                               training=training)\n",
    "      \n",
    "    # Last factored out audio will be encoded as discrete logistic\n",
    "    # The parameters are fixed and no mixture. To implement clean loss\n",
    "    # easier to generate mix of the same discrete logistic\n",
    "    audio = tf.reshape(audio, [audio.shape[0], audio.shape[1] * audio.shape[2], 1])\n",
    "    \n",
    "    last_means = tf.zeros(audio.shape[:-1] + [self.n_logistic_in_mixture])\n",
    "    last_log_scales = tf.zeros(audio.shape[:-1] + [self.n_logistic_in_mixture]) + self.last_log_shift\n",
    "    last_logit = tf.concat([tf.ones(audio.shape), \n",
    "                            tf.zeros(audio.shape[:-1] + [self.n_logistic_in_mixture - 1])],\n",
    "                           axis=2)\n",
    "    \n",
    "    # Append last outputs\n",
    "    output_audio.append(audio)\n",
    "    output_logit.append(last_logit)\n",
    "    output_means.append(last_means)\n",
    "    output_log_scales.append(last_log_scales)\n",
    "    \n",
    "    # Concatenate outputs\n",
    "    output_means = tf.concat(output_means, axis=1)\n",
    "    output_logit = tf.concat(output_logit, axis=1)\n",
    "    output_log_scales = tf.concat(output_log_scales, axis=1)\n",
    "    output_audio = tf.concat(output_audio, axis=1)\n",
    "    \n",
    "    return (output_audio, output_logit, output_means, output_log_scales)\n",
    "    \n",
    "  def generate(self, signal, block):\n",
    "    \n",
    "    n_factorized_channels = ((self.n_group - self.n_remaining_channels) // self.n_early_every)\n",
    "    \n",
    "    target_shape = [self.batch_size,\n",
    "                  (self.hparams[\"segment_length\"] // self.n_group),\n",
    "                  self.n_remaining_channels + (self.n_blocks - block) * n_factorized_channels]\n",
    "    \n",
    "    audio = tf.reshape(signal, target_shape)\n",
    "    \n",
    "    for index in list(reversed(range(self.n_flows)))[(self.n_blocks - block) * self.n_early_every:(self.n_blocks - block + 1) * self.n_early_every]:\n",
    "      audio = self.waveNetIntegerBlocks[index](audio, forward=False)\n",
    "      audio = self.inv1x1ConvPermuteLayers[index](audio, forward=False)\n",
    "      \n",
    "    if block - 1 >= 0:\n",
    "      logits, means, log_scales = self.discreteLogisticMixParametersNets[block-1](audio)\n",
    "    else:\n",
    "      means = tf.zeros([self.batch_size, self.hparams[\"segment_length\"] // self.n_remaining_channels, self.n_logistic_in_mixture])\n",
    "      log_scales = tf.zeros_like(means) + self.last_log_shift\n",
    "      logits = tf.concat([tf.ones([self.batch_size, self.hparams[\"segment_length\"] // self.n_remaining_channels, 1]), \n",
    "                          tf.zeros([self.batch_size, self.hparams[\"segment_length\"] // self.n_remaining_channels, self.n_logistic_in_mixture - 1])],\n",
    "                          axis=2)  \n",
    "    \n",
    "    target_shape = [self.batch_size,-1,1]\n",
    "    \n",
    "    audio = tf.reshape(audio, target_shape)\n",
    "    \n",
    "    means_shape = [self.batch_size, \n",
    "                   (self.hparams[\"segment_length\"] // self.n_group) * n_factorized_channels,\n",
    "                   1]\n",
    "    \n",
    "    return audio, (logits, means, log_scales)\n",
    "  \n",
    "  def infer_craystack(self, inputs, training=None):\n",
    "    \n",
    "    audio, logits, means, log_scales = self.call(inputs=inputs, training=training)\n",
    "    \n",
    "    audio = [(tf.squeeze(x, axis=-1).numpy() * pow(2, 15)) + pow(2, 15) for x in tf.split(audio, self.n_blocks + 1, axis=1)]\n",
    "    all_params = [(tf.transpose(x, [0, 2, 1]).numpy().astype(np.float32), \n",
    "                  tf.transpose(y, [0, 2, 1]).numpy().astype(np.float32), \n",
    "                  tf.transpose(z, [0, 2, 1]).numpy().astype(np.float32)) for x,y,z in zip(tf.split(logits, self.n_blocks + 1, axis=1),\n",
    "                                                                  tf.split(means, self.n_blocks + 1, axis=1),\n",
    "                                                                  tf.split(log_scales, self.n_blocks + 1, axis=1))]\n",
    "    \n",
    "    return audio, all_params\n",
    "  \n",
    "  def generate_craystack(self, x=None, z=None, block=4):\n",
    "    \n",
    "    if block == 4:\n",
    "      means = tf.zeros([self.batch_size, np.prod(self.zdim), self.hparams[\"n_logistic_in_mixture\"]])\n",
    "      log_scales = tf.zeros_like(means) + self.last_log_shift\n",
    "      logits = tf.concat([tf.ones([self.batch_size, np.prod(self.zdim), 1]),\n",
    "                          tf.zeros([self.batch_size, np.prod(self.zdim), self.hparams[\"n_logistic_in_mixture\"] - 1])], axis=2)\n",
    "    else:\n",
    "      x = None if x is None else x\n",
    "      z = tf.convert_to_tensor((z - pow(2, 15)) / pow(2, 15) , dtype=tf.float32)\n",
    "      signal = z if x is None else tf.concat([tf.reshape(z, shape=[self.batch_size, \n",
    "                                               self.hparams['segment_length'] // self.n_group, \n",
    "                                               self.n_remaining_channels]),\n",
    "                                              tf.reshape(x, shape=[self.batch_size, \n",
    "                                              self.hparams['segment_length'] // self.n_group, \n",
    "                                              (self.n_blocks - block) * self.n_remaining_channels])],\n",
    "                                             axis=2)\n",
    "      x, (logits, means, log_scales) = self.generate(signal=signal, block=block)\n",
    "    \n",
    "    logits = tf.transpose(logits, [0,2,1])\n",
    "    means = tf.transpose(means, [0,2,1])\n",
    "    log_scales = tf.transpose(log_scales, [0,2,1])\n",
    "      \n",
    "    return x, (logits.numpy().astype(np.float32), means.numpy().astype(np.float32), log_scales.numpy().astype(np.float32))\n",
    "  \n",
    "  def select_means_and_scales(self, logits, means, log_scales, output_shape):\n",
    "\n",
    "    argmax = tf.argmax(tf.nn.softmax(logits), axis=2)\n",
    "    selector = tf.one_hot(argmax, \n",
    "                          depth=logits.shape[2],\n",
    "                          dtype=tf.float32)\n",
    "\n",
    "    scales = tf.reduce_sum(tf.math.exp(log_scales) * selector, axis=2)\n",
    "    means = tf.reduce_sum(means * selector, axis=2)\n",
    "\n",
    "    return tf.reshape(means, output_shape), tf.reshape(scales, output_shape)\n",
    "  \n",
    "  def set_compression(self):\n",
    "    self.compressing = True\n",
    "    self.batch_size = self.hparams['compress_batch_size']\n",
    "    \n",
    "  def set_training(self):\n",
    "    self.compressing = False\n",
    "    self.batch_size = self.hparams['train_batch_size']\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(WaveGlow, self).get_config()\n",
    "    config.update(hparams = hparams)\n",
    "    \n",
    "    return config\n",
    "  \n",
    "  def sample_from_discretized_mix_logistic(self, logits, means, log_scales, log_scale_min=-8.):\n",
    "    '''\n",
    "    Args:\n",
    "        logits, means, log_scales: Tensor, [batch_size, time_length, n_logistic_in_mixture]\n",
    "    Returns:\n",
    "        Tensor: sample in range of [-1, 1]\n",
    "    \n",
    "    Adapted from pixelcnn++\n",
    "    '''\n",
    "    # sample mixture indicator from softmax\n",
    "    temp = tf.random.uniform(logits.shape, minval=1e-5, maxval=1. - 1e-5)\n",
    "    temp = logits - tf.math.log(-tf.math.log(temp))\n",
    "    argmax = tf.math.argmax(temp, -1)\n",
    "\n",
    "    # [batch_size, time_length] -> [batch_size, time_length, nr_mix]\n",
    "    one_hot = tf.one_hot(argmax, \n",
    "                         depth=self.n_logistic_in_mixture,\n",
    "                         dtype=tf.float32)\n",
    "\n",
    "    # select logistic parameters\n",
    "    means = tf.reduce_sum(means * one_hot, axis=-1)\n",
    "    log_scales = tf.maximum(tf.reduce_sum(\n",
    "      log_scales * one_hot, axis=-1), log_scale_min)\n",
    "\n",
    "    # sample from logistic & clip to interval\n",
    "    # we don't actually round to the nearest 8-bit value when sampling\n",
    "    u = tf.random.uniform(means.shape, minval=1e-5, maxval=1. - 1e-5) \n",
    "    x = means + tf.math.exp(log_scales) * (tf.math.log(u) - tf.math.log(1 - u))\n",
    "    \n",
    "    return tf.minimum(tf.maximum(x, -1. + 1e-5), 1. - 1e-5)\n",
    "  \n",
    "  def total_loss(self, outputs, num_classes=65536, log_scale_min=-7.0):\n",
    "    '''\n",
    "    Discretized mix of logistic distributions loss.\n",
    "    Note that it is assumed that input is scaled to [-1, 1]\n",
    "    Adapted from pixelcnn++\n",
    "    '''\n",
    "\n",
    "    # audio[batch_size, time_length, 1] + unpack parameters: [batch_size, time_length, num_mixtures]\n",
    "    output_audio, logit_probs, means, log_scales = outputs\n",
    "    \n",
    "    # output_audio [batch_size, time_length, 1] -> [batch_size, time_length, num_mixtures]\n",
    "    y = output_audio * tf.ones(shape=[1, 1, self.n_logistic_in_mixture], dtype=self.dtype)\n",
    "\n",
    "    centered_y = y - means\n",
    "    inv_stdv = tf.math.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n",
    "    cdf_plus = tf.nn.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n",
    "    cdf_min = tf.nn.sigmoid(min_in)\n",
    "\n",
    "    log_cdf_plus = plus_in - tf.nn.softplus(plus_in) # log probability for edge case of -32768 (before scaling)\n",
    "    log_one_minus_cdf_min = - tf.nn.softplus(min_in) # log probability for edge case of 32767 (before scaling)\n",
    "\n",
    "    #probability for all other cases\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "\n",
    "    mid_in = inv_stdv * centered_y\n",
    "    #log probability in the center of the bin, to be used in extreme cases\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * tf.nn.softplus(mid_in)\n",
    "    \n",
    "    log_probs = tf.where(y < -0.999, log_cdf_plus,\n",
    "                         tf.where(y > 0.999, log_one_minus_cdf_min,\n",
    "                                  tf.where(cdf_delta > 1e-5,\n",
    "                                           tf.math.log(tf.maximum(cdf_delta, 1e-12)),\n",
    "                                           log_pdf_mid - np.log((num_classes - 1) / 2))))\n",
    "\n",
    "    log_probs = log_probs + tf.nn.log_softmax(logit_probs, axis=-1)\n",
    "    \n",
    "    logistic_loss = -tf.reduce_sum(log_sum_exp(log_probs)) / (y.shape[0] * y.shape[1] * y.shape[2])\n",
    "    \n",
    "    tf.summary.scalar(name='total_loss',\n",
    "                      data=(logistic_loss))\n",
    "    \n",
    "    for block, log_scales in enumerate(tf.split(log_scales, 4, axis=1)[:-1]):\n",
    "      tf.summary.scalar(name=f'mean_log_scales_block{block}',\n",
    "                        data=tf.reduce_mean(log_scales),\n",
    "                        step=tf.summary.experimental.get_step())\n",
    "      tf.summary.scalar(name=f'max_log_scales_block{block}',\n",
    "                        data=tf.reduce_max(log_scales),\n",
    "                        step=tf.summary.experimental.get_step())\n",
    "      tf.summary.scalar(name=f'min_log_scales_block{block}',\n",
    "                        data=tf.reduce_min(log_scales),\n",
    "                        step=tf.summary.experimental.get_step())\n",
    "    \n",
    "    return logistic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craystack",
   "language": "python",
   "name": "craystack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
